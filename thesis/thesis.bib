@inproceedings{attiasSpeechDenoisingDereverberation2000,
  title = {Speech {{Denoising}} and {{Dereverberation Using Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Attias, Hagai and Platt, John and Acero, Alex and Deng, Li},
  date = {2000},
  volume = {13},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/65699726a3c601b9f31bf04019c8593c-Abstract.html},
  urldate = {2025-12-14},
  abstract = {This paper presents a unified probabilistic framework for denoising and  dereverberation of speech signals. The framework transforms the denois(cid:173) ing and dereverberation problems into Bayes-optimal signal estimation.  The  key  idea is  to  use  a  strong  speech  model  that  is  pre-trained on  a  large data set of clean speech.  Computational efficiency is  achieved by  using variational EM, working in  the frequency domain, and employing  conjugate priors.  The framework covers both single and multiple micro(cid:173) phones.  We  apply this  approach to noisy reverberant speech signals and  get results substantially better than standard methods.}
}

@inproceedings{brandsteinUseExplicitSpeech1998,
  title = {On the Use of Explicit Speech Modeling in Microphone Array Applications},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}} '98 ({{Cat}}. {{No}}.{{98CH36181}})},
  author = {Brandstein, M.S.},
  date = {1998},
  volume = {6},
  pages = {3613--3616},
  publisher = {IEEE},
  location = {Seattle, WA, USA},
  doi = {10.1109/ICASSP.1998.679662},
  url = {http://ieeexplore.ieee.org/document/679662/},
  urldate = {2025-12-14},
  abstract = {This paper addresses the limitations of current approaches to distant-talker speech acquisition and advocates the development of techniques which explicitly incorporate the nature of the speech signal (e.g. statistical non-stationarity, method of production, pitch, voicing, formant structure, and source radiator model) into a multi-channel context. The goal is to combine the advantages of spatial ltering achieved through beamforming with knowledge of the desired time-series attributes. The potential utility of such an approach is demonstrated through the application of a multi-channel version of the Dual Excitation speech model.},
  eventtitle = {1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  isbn = {978-0-7803-4428-0},
  langid = {english}
}

@inproceedings{ernstSpeechDereverberationUsing2018,
  title = {Speech {{Dereverberation Using Fully Convolutional Networks}}},
  booktitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Ernst, Ori and Chazan, Shlomo E. and Gannot, Sharon and Goldberger, Jacob},
  date = {2018-09},
  pages = {390--394},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO.2018.8553141},
  url = {https://ieeexplore.ieee.org/document/8553141/},
  urldate = {2025-12-09},
  abstract = {Speech derverberation using a single microphone is addressed in this paper. Motivated by the recent success of the fully convolutional networks (FCN) in many image processing applications, we investigate their applicability to enhance the speech signal represented by short-time Fourier transform (STFT) images. We present two variations: a “U-Net” which is an encoder-decoder network with skip connections and a generative adversarial network (GAN) with U-Net as generator, which yields a more intuitive cost function for training. To evaluate our method we used the data from the REVERB challenge, and compared our results to other methods under the same conditions. We have found that our method outperforms the competing methods in most cases.},
  eventtitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  keywords = {Computer architecture,Gallium nitride,Generative adversarial networks,Reverberation,Spectrogram,Speech enhancement,Task analysis}
}

@article{luoConvTasNetSurpassingIdeal2019,
  title = {Conv-{{TasNet}}: {{Surpassing Ideal Time-Frequency Magnitude Masking}} for {{Speech Separation}}},
  shorttitle = {Conv-{{TasNet}}},
  author = {Luo, Yi and Mesgarani, Nima},
  date = {2019-08},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {27},
  number = {8},
  eprint = {1809.07454},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1256--1266},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2019.2915167},
  url = {http://arxiv.org/abs/1809.07454},
  urldate = {2025-12-09},
  abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, ConvTasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study therefore represents a major step toward the realization of speech separation systems for real-world speech processing technologies.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{luoRealtimeSinglechannelDereverberation2018,
  title = {Real-Time {{Single-channel Dereverberation}} and {{Separation}} with {{Time-domain Audio Separation Network}}},
  booktitle = {Interspeech 2018},
  author = {Luo, Yi and Mesgarani, Nima},
  date = {2018-09-02},
  pages = {342--346},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2018-2290},
  url = {https://www.isca-archive.org/interspeech_2018/luo18c_interspeech.html},
  urldate = {2025-12-05},
  abstract = {We investigate the recently proposed Time-domain Audio Separation Network (TasNet) in the task of real-time singlechannel speech dereverberation. Unlike systems that take timefrequency representation of the audio as input, TasNet learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative autoencoder. We show that by formulating the dereverberation problem as a denoising problem where the direct path is separated from the reverberations, a TasNet denoising autoencoder can outperform a deep LSTM baseline on log-power magnitude spectrogram input in both causal and non-causal settings. We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance.},
  eventtitle = {Interspeech 2018},
  langid = {english}
}

@online{luoTasNetTimedomainAudio2018,
  title = {{{TasNet}}: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation},
  shorttitle = {{{TasNet}}},
  author = {Luo, Yi and Mesgarani, Nima},
  date = {2018-04-18},
  eprint = {1711.00541},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.00541},
  url = {http://arxiv.org/abs/1711.00541},
  urldate = {2025-12-09},
  abstract = {Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@incollection{NAYLOR2014879,
  title = {Chapter 31 - Dereverberation},
  booktitle = {Academic Press Library in Signal Processing: {{Volume}} 4},
  author = {Naylor, Patrick A.},
  editor = {Trussell, Joel and Srivastava, Anuj and Roy-Chowdhury, Amit K. and Srivastava, Ankur and Naylor, Patrick A. and Chellappa, Rama and Theodoridis, Sergios},
  date = {2014},
  series = {Academic Press Library in Signal Processing},
  volume = {4},
  pages = {879--914},
  publisher = {Elsevier},
  issn = {2351-9819},
  doi = {10.1016/B978-0-12-396501-1.00031-5},
  url = {https://www.sciencedirect.com/science/article/pii/B9780123965011000315},
  abstract = {Dereverberation employs signal processing techniques in order to reduce the degrading effects of reverberation in speech and music signals. Levels of reverberation in a signal can be high when the effects of the acoustic space in which the sound is captured are very noticeable and reverberation generally increases as the distance of the microphone from the sound source increases. High levels of reverberation may be attractive in some signals, such as in some kinds of music, but can degrade listening comfort and also intelligibility in speech signals, particularly in combination with noise or other degrading artefacts. After an overview of the topic of dereverberation, this chapter discusses applications of reverberation and dereverberation in order to set the scene for the later sections. Room acoustics and methods for measuring reverberation in rooms and from reverberant signals are then presented in overview. Three main classes of methods of dereverberation are then defined. These are methods based on spatial filtering, methods based on speech enhancement and channel-based methods. The methods of each class are introduced and some details are given together with references to further reading.}
}

@article{neumanCombinedEffectsNoise2010,
  title = {Combined {{Effects}} of {{Noise}} and {{Reverberation}} on {{Speech Recognition Performance}} of {{Normal-Hearing Children}} and {{Adults}}},
  author = {Neuman, Arlene C. and Wroblewski, Marcin and Hajicek, Joshua and Rubinstein, Adrienne},
  date = {2010-06},
  journaltitle = {Ear and Hearing},
  volume = {31},
  number = {3},
  pages = {336},
  issn = {1538-4667},
  doi = {10.1097/AUD.0b013e3181d3d514},
  url = {https://journals.lww.com/ear-hearing/abstract/2010/06000/combined_effects_of_noise_and_reverberation_on.4.aspx},
  urldate = {2025-12-14},
  abstract = {Objectives:~           The purpose of this study is to determine how combinations of noise levels and reverberation typical of ranges found in current classrooms will affect speech recognition performance of typically developing children with normal speech, language, and hearing and to compare their performance with that of adults with normal hearing. Speech recognition performance was measured using the Bamford-Kowal-Bench Speech in Noise test. A virtual test paradigm represented the signal reaching a student seated in the back of a classroom with a volume of 228 m3 and with varied reverberation time (0.3, 0.6, and 0.8 sec). The signal to noise ratios required for 50\% performance (SNR-50) and for 95\% performance were determined for groups of children aged 6 to 12 yrs and a group of young adults with normal hearing.           Design:~           This is a cross-sectional developmental study incorporating a repeated measures design. Experimental variables included age and reverberation time. A total of 63 children with normal hearing and typically developing speech and language and nine adults with normal hearing were tested. Nine children were included in each age group (6, 7, 8, 9, 10, 11, and 12 yrs).           Results:~           The SNR-50 increased significantly with increased reverberation and decreased significantly with increasing age. On average, children required positive SNRs for 50\% performance, whereas thresholds for adults were close to 0 dB or {$<$}0 dB for the conditions tested. When reverberant SNR-50 was compared with adult SNR-50 without reverberation, adults did not exhibit an SNR loss, but children aged 6 to 8 yrs exhibited a moderate SNR loss and children aged 9 to 12 yrs exhibited a mild SNR loss. To obtain average speech recognition scores of 95\% at the back of the classroom, an SNR ≥10 dB is required for all children at the lowest reverberation time, of ≥12 dB for children up to age 11 yrs at the 0.6-sec reverberant condition, and of ≥15 dB for children aged 7 to 11 yrs at the 0.8-sec condition. The youngest children require even higher SNRs in the 0.8-sec condition.           Conclusions:~           Results highlight changes in speech recognition performance with age in elementary school children listening to speech in noisy, reverberant classrooms. The more reverberant the environment, the better the SNR required. The younger the child, the better the SNR required. Results support the importance of attention to classroom acoustics and emphasize the need for maximizing SNR in classrooms, especially in classrooms designed for early childhood grades.},
  langid = {american}
}

@article{puglisiEffectReverberationNoise2021,
  title = {Effect of Reverberation and Noise Type on Speech Intelligibility in Real Complex Acoustic Scenarios},
  author = {Puglisi, Giuseppina Emma and Warzybok, Anna and Astolfi, Arianna and Kollmeier, Birger},
  date = {2021-10-15},
  journaltitle = {Building and Environment},
  shortjournal = {Building and Environment},
  volume = {204},
  pages = {108137},
  issn = {0360-1323},
  doi = {10.1016/j.buildenv.2021.108137},
  url = {https://www.sciencedirect.com/science/article/pii/S0360132321005382},
  urldate = {2025-12-14},
  abstract = {Enhanced speech intelligibility in realistic acoustic scenarios guarantees effective communication. Noise and reverberation degrade speech intelligibility, but the combined effect of reverberation, informational noise and position of target, listener and noise source on speech intelligibility still needs to insights. This work investigates the effect of real complex acoustic scenarios on speech intelligibility for adults. In two primary-school classrooms with reverberation time of 0.4~s and 3.1~s, receivers were located on axis with the target at increasing distances, and noise sources were both co-located and spatially separated from the target at different distances providing various degrees of energetic and informational masking. Noise level was maintained constant and set at 60~dB at the receiver's position, regardless of the noise source distance. The longer reverberation time resulted in worse speech recognition thresholds (SRT80s) by 6~dB on average. The competitive effect of informational noise resulted in increased SRT80s by 7~dB on average, in comparison to energetic noise. The SRT80 increases by \textasciitilde 2~dB with doubling the target-to-receiver distance when the noise source is close to the receiver, accounting for both reverberations and noise types. The spatial release from masking resulted in improved speech intelligibility by up to \textasciitilde 3~dB when the noise source is 1~m far from the receiver for energetic masker for low reverberation and, unexpectedly, for the informational masker in high reverberation. This may indicate that a perceptual segregation mechanism sorts out competing voices of informational masker according to their directions in least favorable listening situations.},
  keywords = {Binaural hearing,Classroom acoustics,Complex acoustic scenarios,Informational masking,Spatial release from masking,Speech intelligibility}
}

@inproceedings{schmidMeasuringJustNoticeable2024,
  title = {Measuring the {{Just Noticeable Difference}} for {{Audio Latency}}},
  booktitle = {Audio {{Mostly}} 2024 - {{Explorations}} in {{Sonic Cultures}}},
  author = {Schmid, Andreas and Ambros, Maria and Bogon, Johanna and Wimmer, Raphael},
  date = {2024-09-18},
  pages = {325--331},
  publisher = {ACM},
  location = {Milan Italy},
  doi = {10.1145/3678299.3678331},
  url = {https://dl.acm.org/doi/10.1145/3678299.3678331},
  urldate = {2025-12-09},
  abstract = {All parts of an audio processing chain introduce latency. Previous studies have shown that high audio latency may negatively impact human performance in different scenarios, e.g., when performing live music or when interacting with real-time human-computer systems. However, is not yet known where the human perception threshold for audio latency lies, i.e., what the lowest amount of latency is that musicians might notice. Therefore, we conducted a user study (n=37) using the PEST method to estimate the just noticeable difference (JND) for audio latency under different base latency settings. Our results suggest that base latency influences the perception threshold in a non-linear manner: Participants achieved a mean JND of 49 ms for a base latency of 0 ms, 27 ms for a base latency of 64 ms, and 77 ms for a base latency of 512 ms. Furthermore, the JND was lower for participants with high musical sophistication.},
  eventtitle = {{{AM}} '24: {{Audio Mostly}} 2024 - {{Explorations}} in {{Sonic Cultures}}},
  isbn = {979-8-4007-0968-5},
  langid = {english}
}

@article{schwarzRealTimeDereverberationDeep2015,
  title = {Real-{{Time Dereverberation}} for {{Deep Neural Network}}  {{Speech Recognition}}},
  author = {Schwarz, A and Huemmer, C and Maas, R and Kellermann, W},
  date = {2015},
  abstract = {We evaluate a real-time multi-channel dereverberation method for the application to speech recognition with deep neural networks (DNN). The dereverberation method is based on modeling the reverberated signal as a mixture of a fully coherent direct path signal and a diffuse reverberation component, and estimating the coherentto-diffuse power ratio (CDR) from the spatial coherence of the signals. The method can operate in real-time, i.e., without requiring processing of entire utterances. We compare CDR estimators which are “blind”, i.e., do not require information about the direction of arrival (DOA) of the target signal, with estimators which make use of a DOA estimate. The impact of the dereverberation method on speech recognition accuracy with different DNN-based acoustic models is investigated with the REVERB challenge corpus and the Kaldi speech recognition toolkit.},
  langid = {english}
}

@article{traerStatisticsNaturalReverberation2016,
  title = {Statistics of Natural Reverberation Enable Perceptual Separation of Sound and Space},
  author = {Traer, James and McDermott, Josh H.},
  date = {2016-11-29},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {48},
  pages = {E7856-E7865},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1612524113},
  url = {https://www.pnas.org/doi/10.1073/pnas.1612524113},
  urldate = {2025-12-14},
  abstract = {In everyday listening, sound reaches our ears directly from a source as well as indirectly via reflections known as reverberation. Reverberation profoundly distorts the sound from a source, yet humans can both identify sound sources and distinguish environments from the resulting sound, via mechanisms that remain unclear. The core computational challenge is that the acoustic signatures of the source and environment are combined in a single signal received by the ear. Here we ask whether our recognition of sound sources and spaces reflects an ability to separate their effects and whether any such separation is enabled by statistical regularities of real-world reverberation. To first determine whether such statistical regularities exist, we measured impulse responses (IRs) of 271 spaces sampled from the distribution encountered by humans during daily life. The sampled spaces were diverse, but their IRs were tightly constrained, exhibiting exponential decay at frequency-dependent rates: Mid frequencies reverberated longest whereas higher and lower frequencies decayed more rapidly, presumably due to absorptive properties of materials and air. To test whether humans leverage these regularities, we manipulated IR decay characteristics in simulated reverberant audio. Listeners could discriminate sound sources and environments from these signals, but their abilities degraded when reverberation characteristics deviated from those of real-world environments. Subjectively, atypical IRs were mistaken for sound sources. The results suggest the brain separates sound into contributions from the source and the environment, constrained by a prior on natural reverberation. This separation process may contribute to robust recognition while providing information about spaces around us.}
}

@online{valinDereverbNotDereverb2022,
  title = {To {{Dereverb Or Not}} to {{Dereverb}}? {{Perceptual Studies On Real-Time Dereverberation Targets}}},
  shorttitle = {To {{Dereverb Or Not}} to {{Dereverb}}?},
  author = {Valin, Jean-Marc and Giri, Ritwik and Venkataramani, Shrikant and Isik, Umut and Krishnaswamy, Arvindh},
  date = {2022-06-16},
  eprint = {2206.07917},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2206.07917},
  url = {http://arxiv.org/abs/2206.07917},
  urldate = {2025-12-05},
  abstract = {In real life, room effect, also known as room reverberation, and the present background noise degrade the quality of speech. Recently, deep learning-based speech enhancement approaches have shown a lot of promise and surpassed traditional denoising and dereverberation methods. It is also well established that these state-of-the-art denoising algorithms significantly improve the quality of speech as perceived by human listeners. But the role of dereverberation on subjective (perceived) speech quality, and whether the additional artifacts introduced by dereverberation cause more harm than good are still unclear. In this paper, we attempt to answer these questions by evaluating a state of the art speech enhancement system in a comprehensive subjective evaluation study for different choices of dereverberation targets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}
